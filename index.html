<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="FOCUS is a training-free visual cropping method that leverages MLLM-internal representations to guide the search for the most relevant image region, achieving strong performance across fine-grained VQA datasets with improved efficiency.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="MLLM, vision-language models, visual question answering, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Liangyu Zhong, Fabio Rosenthal, Joachim Sicking, Fabian Hüger, Thorsten Bagdonat, Hanno Gottschalk, Leo Schwinn">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="CARIAD SE, TU Berlin, TU Munich, Volkswagen AG">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="FOCUS is a training-free visual cropping method that leverages MLLM-internal representations to guide the search for the most relevant image region, achieving strong performance across fine-grained VQA datasets with improved efficiency.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://focus-mllm-vqa.github.io/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://github.com/FOCUS-MLLM-VQA/focus-mllm-vqa.github.io/blob/main/static/images/focus_icon.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering - NeurIPS 2025">
  <meta property="article:published_time" content="2025-10-22T00:00:00.000Z">
  <meta property="article:author" content="Liangyu Zhong, Fabio Rosenthal, Joachim Sicking, Fabian Hüger, Thorsten Bagdonat, Hanno Gottschalk, Leo Schwinn">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="MLLM">
  <meta property="article:tag" content="Visual Question Answering">

  <!-- Twitter -->
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering">
  <meta name="citation_author" content="Zhong, Liangyu">
  <meta name="citation_author" content="Rosenthal, Fabio">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="NeurIPS">
  <meta name="citation_pdf_url" content="https://arxiv.org/abs/2506.21710">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering | NeurIPS 2025</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/focus_icon.png">
  <link rel="apple-touch-icon" href="static/images/focus_icon.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <div style="display: flex; align-items: flex-start; justify-content: center; gap: 1.5em;">
              <div style="display: flex; flex-direction: column; align-items: center;">
                <img src="static/images/focus_icon.png" alt="FOCUS icon" style="max-width: 10em; height: auto; margin-bottom: 0.5em;" loading="lazy">
              </div>
              <h1 class="title is-1 publication-title" style="text-align: left; ">
                FOCUS: Internal MLLM Representations
                for Efficient Fine-Grained
                VQA
              </h1>
            </div>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://de.linkedin.com/in/liangyu-zhong-604967123" target="_blank">Liangyu Zhong</a><sup>*1,3</sup>,</span>
                <span class="author-block">
                  <a href="https://de.linkedin.com/in/fabio-rosenthal-677869153" target="_blank">Fabio Rosenthal</a><sup>*2,4</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=DHDXHv8AAAAJ&hl=de" target="_blank">Joachim Sicking</a><sup>3</sup>,
                  </span>
                      <span class="author-block">
                    <a href="https://de.linkedin.com/in/fabianhueger" target="_blank"> Fabian Hüger</a></a><sup>3</sup>,
                  </span>
                      </span>
                      <span class="author-block">
                    <a href="https://openreview.net/profile?id=~Thorsten_Bagdonat1" target="_blank"> Thorsten Bagdonat</a><sup>4</sup>,
                  </span>
                    <span class="author-block">
                    <a href="https://de.linkedin.com/in/hanno-gottschalk-9371a4232" target="_blank"> Hanno Gottschalk</a><sup>1</sup>,
                  </span>
                                    </span>
                    <span class="author-block">
                    <a href="https://de.linkedin.com/in/leo-schwinn-34a61623b" target="_blank"> Leo Schwinn</a><sup>2</sup>,
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block"><sup>1</sup>Technical University of Berlin, <sup>2</sup>Technical University of Munich, <sup>3</sup>CARIAD SE, <sup>4</sup>Volkswagen AG
                      <br><br>NeurIPS 2025 main track</span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2506.21710" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.21710" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="hero-body">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            While Multimodal Large Language Models (MLLMs) offer strong perception and
reasoning capabilities for image-text input, Visual Question Answering (VQA)
focusing on small image details still remains a challenge. Although visual cropping
techniques seem promising, recent approaches have several limitations: the need
for task-specific fine-tuning, low efficiency due to uninformed exhaustive search,
or incompatibility with efficient attention implementations. We address these
shortcomings by proposing a training-free visual cropping method, dubbed FOCUS,
that leverages MLLM-internal representations to guide the search for the most
relevant image region. This is accomplished in four steps: first, we identify the
target object(s) in the VQA prompt; second, we compute an object relevance map
using the key-value (KV) cache; third, we propose and rank relevant image regions
based on the map; and finally, we perform the fine-grained VQA task using the top-ranked region. As a result of this informed search strategy, FOCUS achieves strong
performance across four fine-grained VQA datasets and two types of MLLMs. It
outperforms three popular visual cropping methods in both accuracy and efficiency,
and matches the best-performing baseline, ZoomEye, while requiring 3 – 6.5×
less compute.
          </p>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>
<!-- End paper abstract -->
<section class="hero is-small">
  <div class="hero-body">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Fine-Grained VQA is Underexplored</h2>
         <p>
              <img src="static/images/motivation_1.svg" alt="Motivation" class="blend-img-background center-image" style="max-width: 100%; height: auto;" loading="lazy">
            </p>
            <p class="has-text-justified">
              While MLLMs demonstrate impressive capabilities on regular  VQA tasks, they often struggle with fine-grained VQA that requires attention to small, localized details in images. 
              We highlight the difference between fine-grained and regular VQA in Fig.(a). Recent MLLMs such as LLaVA-OneVision try to address this issue by employing
              processing additional local crops extracted from the original images. However, their effectiveness on fine-grained VQA tasks still remains limited. <br><br>
              An orthogonal research direction to address the limitations of MLLMs in capturing fine details in
high-resolution images are visual cropping approaches, which seek to pass only
relevant image regions to the MLLM. However, popular visual cropping techniques like SEAL,
DC2, ZoomEye, and ViCrop suffer from one or more of the following limitations (see Fig. (b)):
(1) reliance on task-specific fine-tuning of MLLMs for fine-grained VQA, (2) use of inefficient,
exhaustive tree search algorithms, and (3) dependence on Q-K attention weight and therefore, being incompatible with efficient attention implementations such as
FlashAttention.<br>
            </p>
        </div>
      </div>
    </div>
    </div>
  </div>
</section>

<section class="hero is-small is-light">
   <div class="hero-body">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3" style="text-align: left;">FOCUS: <span style="text-decoration: underline;">F</span>ine-Grained Visual <span style="text-decoration: underline;">O</span>bject <span style="text-decoration: underline;">C</span>ropping <br><span style="text-decoration: underline;">U</span>sing Cached Token <span style="text-decoration: underline;">S</span>imilarity</h2>
         <p>
              <img src="static/images/flow_chart1.svg" alt="Method" class="blend-img-background center-image" style="max-width: 100%; height: auto;" loading="lazy">
            </p>
            <p class="has-text-justified">
            To tackle limitation (1), FOCUS leverages the internal representations of MLLMs, specifically
their key-value (KV) caches, to localize
question-relevant image regions in a training-free manner—unlike the SEAL technique.
Moreover, to mitigate limitation (2), our method
includes textual clues to enable object-aware
localization without exhaustive cropping of the
image, thereby improving the algorithmic efficiency—different from DC2
and ZoomEye. To overcome limitation (3), FOCUS
utilizes the cached value features readily available during inference, making it natively compatible with efficient attention implementations
—unlike ViCrop that depends on full attention weights. <br><br>Specifically, FOCUS combines these
components as follows: for each VQA question, we first identify the target object(s) in the question
prompt. Second, we construct an object relevance map using cosine similarity between the cached
text tokens of the target object(s) and the cached image tokens, and then propose relevant regions
based on this map. Third, we rank the proposed image regions based on the existence confidence of
the target object in each region. Finally, we perform VQA solely based on the image region with the
highest confidence. Note that FOCUS is compatible with both global- and global-local-view MLLMs. <br>
          </p>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Results</h2>
         <p>
              <img src="static/images/pareto.svg" alt="Main results" class="blend-img-background center-image" style="max-width: 100%; height: auto;" loading="lazy">
            </p>
            <p class="has-text-justified">
We conduct experiments with LLaVA-1.5 and LLaVA-OneVision on V*Bench, HRBench-4K, and HRBench-8K. FOCUS with LLaVA-1.5 achieves an accuracy of 72.77% on V*Bench,
51.75% on HRBench-4K, and 45.00% on HRBench-8K. FOCUS with LLaVA-OneVision achieves an accuracy of 92.15% on V*Bench, 71.13% on HRBench-4K, and 69.63% on HRBench-8K. Overall, FOCUS outperforms three popular visual cropping methods (<a href="https://arxiv.org/abs/2312.14135" target="_blank">SEAL</a>, <a href="https://arxiv.org/abs/2502.17422" target="_blank">ViCrop</a> and <a href="https://arxiv.org/abs/2408.15556" target="_blank">DC2</a>) in both accuracy and efficiency, and matches the best-performing baseline, <a href="https://arxiv.org/abs/2411.16044" target="_blank">ZoomEye</a>, while requiring 3 – 6.5× less compute. 
          </p>
            <p class="has-text-justified">
              <img src="static/images/MME-R.png" alt="MME-R results" class="blend-img-background center-image" style="max-width: 100%; height: auto;" loading="lazy">
                          <p class="has-text-justified">
Furthermore, we perform additional evaluation on MME-RealWorld-Lite. FOCUS outperforms the vanilla baseline on most sub-tasks. ZoomEye
and our method have strengths in different domains. FOCUS is better for reasoning, while ZoomEye is better at perception tasks. Still, our method is on average 5.47× more efficient than ZoomEye.
          </p>

              <div style="display: flex; justify-content: center;">
                <img src="static/images/qwen25VL.png" alt="qwen25vl" class="blend-img-background center-image" style="max-width: 50%; height: auto;" loading="lazy">
              </div>
              <p class="has-text-justified">
                Qwen-2.5-VL processes high-resolution images with native resolution, thereby preserving spatial details more effectively. We evaluate FOCUS with Qwen-2.5-VL and find state-of-the-art accuracy on HRBench-4K and HRBench-8K, 79.25% and 76.25%, respectively. This confirms the
compatibility of FOCUS with different MLLM architectures.
            </p>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>


<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Qualitative examples</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/qualitative_examples_1.svg" alt="First qualitative_examples_1" loading="lazy"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
           We provide some exemplary inferences with our method for single-target tasks with LLaVA-1.5 (I) and multiple-target tasks with LLaVA-OneVision (II). The Ground Truth (GT)
locations are highlighted in red in the original image. Further, we show the detected image regions and their
locations in the object relevance map. Note that the object relevance maps corresponds to the original images.

        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/qualitative_examples_2.svg" alt="First qualitative_examples_2" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Further qualitative examples of FOCUS with LLaVA-1.5. We provide examples for single-object (I),
multi-object (II), a type-2 question (III), and a failure case (IV). Note that we do not adjust the aspect ratio of
the images for LLaVA-1.5. Therefore, there are some padding areas in the object relevance maps. Additionally,
we manually highlight the relevant regions in the original image to facilitate easier localization of the ground
truth area for the reader and these annotations are not included in the input for the MLLM.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/qualitative_examples_3.svg" alt="First qualitative_examples_3" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Further qualitative examples of FOCUS with LLaVA-OneVision. We provide examples for single-object (I), multi-object (II), a type-2 question (III), and a failure case (IV). Note that we manually highlight the
relevant regions in the original image to facilitate easier localization of the ground truth area for the reader and
these annotations are not included in the input for the MLLM.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->








<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{zhong2025focus,
  author       = {Liangyu Zhong and
                  Fabio Rosenthal and
                  Joachim Sicking and
                  Fabian H{\"{u}}ger and
                  Thorsten Bagdonat and
                  Hanno Gottschalk and
                  Leo Schwinn},
  title        = {{FOCUS:} Internal {MLLM} Representations for Efficient Fine-Grained
                  Visual Question Answering},
  journal      = {arXiv},
  volume       = {abs/2506.21710},
  year         = {2025},
  url          = {https://doi.org/10.48550/arXiv.2506.21710},
  doi          = {10.48550/ARXIV.2506.21710},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
